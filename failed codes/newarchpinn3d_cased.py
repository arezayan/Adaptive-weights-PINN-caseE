# -*- coding: utf-8 -*-
"""newArchPINN3D_caseD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12CqIbQ_EtU-o8oxU9aCyOQ67yn7mvtP3
"""

import torch
import numpy as np
import pandas as pd
#import foamFileOperation
from matplotlib import pyplot as plt
#from mpl_toolkits.mplot3d import Axes3D
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import pdb
#from torchvision import datasets, transforms
import csv
from torch.utils.data import DataLoader, TensorDataset,RandomSampler
from math import exp, sqrt,pi
import time
#import vtk
#from vtk.util import numpy_support as VN

# Network for u component
class NetU(nn.Module):
    def __init__(self, layers):
        super(NetU, self).__init__()
        self.layers = nn.ModuleList()

        for i in range(len(layers) - 1):
            layer = nn.Linear(layers[i], layers[i + 1])
            nn.init.xavier_uniform_(layer.weight)  # Xavier initialization for weights
            nn.init.zeros_(layer.bias)             # Initialize biases to zero
            nn.init.kaiming_normal(layer.weight, nonlinearity = 'relu')
            self.layers.append(layer)

    def forward(self, x):
        for i in range(len(self.layers) - 1):
            x = torch.tanh(self.layers[i](x))
        x = self.layers[-1](x)
        return x
hidden_dim = 20
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
layers = [3, hidden_dim, hidden_dim, hidden_dim, hidden_dim, hidden_dim, hidden_dim, hidden_dim, hidden_dim,  1]  # Input: (x, y, z), Output: (u)


# Network for v component
class NetV(nn.Module):
    def __init__(self, layers):
        super(NetV, self).__init__()
        self.layers = nn.ModuleList()

        for i in range(len(layers) - 1):
            layer = nn.Linear(layers[i], layers[i + 1])
            nn.init.xavier_uniform_(layer.weight)  # Xavier initialization for weights
            nn.init.zeros_(layer.bias)             # Initialize biases to zero
            nn.init.kaiming_normal(layer.weight, nonlinearity = 'relu')
            self.layers.append(layer)

    def forward(self, x):
        for i in range(len(self.layers) - 1):
            x = torch.tanh(self.layers[i](x))
        x = self.layers[-1](x)
        return x
hidden_dim = 20
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
layers = [3, hidden_dim, hidden_dim, hidden_dim, hidden_dim, hidden_dim, hidden_dim, hidden_dim, hidden_dim,  1]  # Input: (x, y, z), Output: (u)

# Network for w component
class NetW(nn.Module):
    def __init__(self, layers):
        super(NetW, self).__init__()
        self.layers = nn.ModuleList()

        for i in range(len(layers) - 1):
            layer = nn.Linear(layers[i], layers[i + 1])
            nn.init.xavier_uniform_(layer.weight)  # Xavier initialization for weights
            nn.init.zeros_(layer.bias)             # Initialize biases to zero
            nn.init.kaiming_normal(layer.weight, nonlinearity = 'relu')
            self.layers.append(layer)

    def forward(self, x):
        for i in range(len(self.layers) - 1):
            x = torch.tanh(self.layers[i](x))
        x = self.layers[-1](x)
        return x
hidden_dim = 20
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
layers = [3, hidden_dim, hidden_dim, hidden_dim, hidden_dim, hidden_dim, hidden_dim, hidden_dim, hidden_dim,  1]  # Input: (x, y, z), Output: (u)


# Network for Pressure ,TKE and Epsilon
class NetP(nn.Module):
    def __init__(self, layers):
        super(NetP, self).__init__()
        self.layers = nn.ModuleList()

        for i in range(len(layers) - 1):
            layer = nn.Linear(layers[i], layers[i + 1])
            nn.init.xavier_uniform_(layer.weight)  # Xavier initialization for weights
            nn.init.zeros_(layer.bias)             # Initialize biases to zero
            nn.init.kaiming_normal(layer.weight, nonlinearity = 'relu')
            self.layers.append(layer)

    def forward(self, x):
        for i in range(len(self.layers) - 1):
            x = torch.tanh(self.layers[i](x))
        x = self.layers[-1](x)
        return x
hidden_dim = 20
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
layers = [3, hidden_dim, hidden_dim, hidden_dim, hidden_dim, hidden_dim, hidden_dim, hidden_dim, hidden_dim,  1]  # Input: (x, y, z), Output: (u)




netu  = NetU(layers).to(device)
netv  = NetV(layers).to(device)
netw  = NetW(layers).to(device)
netpke  = NetP(layers).to(device)

#Import Data
# True dataset which is exact data
data = pd.read_csv('caseE_Data.csv')
data = (data - data.min()) / (data.max() - data.min())
data['z'] = 2
data['v'] = 0
data['w'] = 0
x = torch.tensor(data[['x']].values, dtype=torch.float32 , requires_grad=True).to(device)
y = torch.tensor(data[['y']].values, dtype=torch.float32 , requires_grad=True).to(device)
z = torch.tensor(data[['z']].values, dtype=torch.float32 , requires_grad=True).to(device)
u_exact = torch.tensor(data[['u']].values, dtype=torch.float32 ).to(device)
v_exact = torch.tensor(data[['v']].values, dtype=torch.float32 ).to(device)
w_exact = torch.tensor(data[['w']].values, dtype=torch.float32 ).to(device)


# BC dataset which is exact data
data_bc = pd.read_csv('caseE_BC.csv')
data_bc = (data_bc - data_bc.min()) / (data_bc.max() - data_bc.min())

data_bc['u'] = 0
data_bc['v'] = 0
data_bc['w'] = 0

x_b = torch.tensor(data_bc[['x']].values, dtype=torch.float32 , requires_grad=True).to(device)
y_b = torch.tensor(data_bc[['y']].values, dtype=torch.float32 , requires_grad=True).to(device)
z_b = torch.tensor(data_bc[['z']].values, dtype=torch.float32 , requires_grad=True).to(device)
u_b = torch.tensor(data_bc[['u']].values, dtype=torch.float32 ).to(device)
v_b = torch.tensor(data_bc[['v']].values, dtype=torch.float32 ).to(device)
w_b = torch.tensor(data_bc[['w']].values, dtype=torch.float32 ).to(device)


# Valid dataset which is exact data
data_v = pd.read_csv('test_cloud1_U_caseE.csv')
data_v = (data_v - data_v.min()) / (data_v.max() - data_v.min())
data_v[['z']] = 2
data_v[['v']] = 0
data_v[['w']] = 0


x_v = torch.tensor(data_v[['x']].values, dtype=torch.float32 , requires_grad=True).to(device)
y_v = torch.tensor(data_v[['y']].values, dtype=torch.float32 , requires_grad=True).to(device)
z_v = torch.tensor(data_v[['z']].values, dtype=torch.float32 , requires_grad=True).to(device)
u_v = torch.tensor(data_v[['u']].values, dtype=torch.float32 ).to(device)
v_v = torch.tensor(data_v[['v']].values, dtype=torch.float32 ).to(device)
w_v = torch.tensor(data_v[['w']].values, dtype=torch.float32 ).to(device)

#Dynamic Weight calculation

def compute_w_data(x,y,z):
  up= netu(torch.cat((x, y, z), dim=1))
  vp= netv(torch.cat((x, y, z), dim=1))
  wp= netw(torch.cat((x, y, z), dim=1))



  ku =(up - u_exact) / (u_exact + 1e-16)
  kv =(vp - v_exact) / (v_exact + 1e-16)
  kw =(wp - w_exact) / (w_exact + 1e-16)


  ku_mean = torch.mean(ku)
  kv_mean = torch.mean(kv)
  kw_mean = torch.mean(kw)


  wu = (torch.mean(ku) / min(kv_mean , kw_mean) ) **2
  wv = (torch.mean(kv) / min(ku_mean , kw_mean) ) **2
  ww = (torch.mean(kw) / min(ku_mean , kv_mean) ) **2


  return wu,wv, ww


def compute_weight_PDE(momentum_loss , continuity_loss , loss_k , loss_epsilon, prev_mom = torch.tensor(1E11, requires_grad=False), prev_cont = torch.tensor(1E11, requires_grad=False), prev_k = torch.tensor(1E11, requires_grad=False) , prev_epsilon = torch.tensor(1E11, requires_grad=False)):
    # Calculate the magnitude of gradients or loss values and adjust the weights
    weight_momentum = torch.mean(prev_mom) / (torch.mean(momentum_loss) + 1e-8)
    weight_continuity = torch.mean(prev_cont) / (torch.mean(continuity_loss) + 1e-8)
    weight_k = torch.mean(prev_k) / (torch.mean(loss_k) + 1e-8)
    weight_epsilon = torch.mean(prev_epsilon) / (torch.mean(loss_epsilon) + 1e-8)

    # Normalize the weights
    total_weight = weight_momentum + weight_continuity + weight_k + weight_epsilon
    weight_momentum /= total_weight
    weight_continuity /= total_weight
    weight_k /= total_weight
    weight_epsilon /= total_weight

    return weight_momentum, weight_continuity , weight_k , weight_epsilon

# Physics-Informed Neural Network Loss Calculation (Steady-State Navier-Stokes)
#def compute_loss_PDE(model, x, y, z,  mu=0.01):
def compute_loss_PDE(netu ,netv ,netw,netpke, x, y,z, nu=1.5E-5 / 1.225 , C_mu=0.09, C1_eps=1.44, C2_eps=1.92, sigma_k=1.0, sigma_eps=1.3):
    x = x.requires_grad_(True)
    y = y.requires_grad_(True)
    z = z.requires_grad_(True)

    # Forward pass: Predict u, v, w (velocity) and p (pressure)

    u = netu(torch.cat((x, y, z), dim=1))
    v = netv(torch.cat((x, y, z), dim=1))
    w = netw(torch.cat((x, y, z), dim=1))
    p = netpke(torch.cat((x, y, z), dim=1))
    k = netpke(torch.cat((x, y, z), dim=1))
    epsilon = netpke(torch.cat((x, y, z), dim=1))


    # Calculate gradients
    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]
    u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u), create_graph=True)[0]
    u_z = torch.autograd.grad(u, z, grad_outputs=torch.ones_like(u), create_graph=True)[0]

    v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), create_graph=True)[0]
    v_y = torch.autograd.grad(v, y, grad_outputs=torch.ones_like(v), create_graph=True)[0]
    v_z = torch.autograd.grad(v, z, grad_outputs=torch.ones_like(v), create_graph=True)[0]

    w_x = torch.autograd.grad(w, x, grad_outputs=torch.ones_like(w), create_graph=True)[0]
    w_y = torch.autograd.grad(w, y, grad_outputs=torch.ones_like(w), create_graph=True)[0]
    w_z = torch.autograd.grad(w, z, grad_outputs=torch.ones_like(w), create_graph=True)[0]

    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]
    u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y), create_graph=True)[0]
    u_zz = torch.autograd.grad(u_z, z, grad_outputs=torch.ones_like(u_z), create_graph=True)[0]

    v_xx = torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(v_x), create_graph=True)[0]
    v_yy = torch.autograd.grad(v_y, y, grad_outputs=torch.ones_like(v_y), create_graph=True)[0]
    v_zz = torch.autograd.grad(v_z, z, grad_outputs=torch.ones_like(v_z), create_graph=True)[0]

    w_xx = torch.autograd.grad(w_x, x, grad_outputs=torch.ones_like(w_x), create_graph=True)[0]
    w_yy = torch.autograd.grad(w_y, y, grad_outputs=torch.ones_like(w_y), create_graph=True)[0]
    w_zz = torch.autograd.grad(w_z, z, grad_outputs=torch.ones_like(w_z), create_graph=True)[0]

    p_x = torch.autograd.grad(p, x, grad_outputs=torch.ones_like(p), create_graph=True)[0]
    p_y = torch.autograd.grad(p, y, grad_outputs=torch.ones_like(p), create_graph=True)[0]
    p_z = torch.autograd.grad(p, z, grad_outputs=torch.ones_like(p), create_graph=True)[0]

      # Compute the gradients for k and ε
    k_x = torch.autograd.grad(k, x, grad_outputs=torch.ones_like(k), create_graph=True)[0]
    k_y = torch.autograd.grad(k ,y ,grad_outputs=torch.ones_like(k), create_graph=True)[0]
    k_z = torch.autograd.grad(k ,z ,grad_outputs=torch.ones_like(k), create_graph=True)[0]

    epsilon_x = torch.autograd.grad(epsilon, x, grad_outputs=torch.ones_like(epsilon), create_graph=True)[0]
    epsilon_y = torch.autograd.grad(epsilon, y, grad_outputs=torch.ones_like(epsilon), create_graph=True)[0]
    epsilon_z = torch.autograd.grad(epsilon, z, grad_outputs=torch.ones_like(epsilon), create_graph=True)[0]



    #Turbulent viscosity
    nu_t = C_mu * (k ** 2 / epsilon)
    nu_eff = nu + nu_t  # Effective viscosity

    # Navier-Stokes equations

    f_u = u*u_x + v*u_y + w*u_z + p_x - nu_eff * (u_xx + u_yy + u_zz)
    f_v = u*v_x + v*v_y + w*v_z + p_y - nu_eff * (v_xx + v_yy + v_zz)
    f_w = u*w_x + v*w_y + w*w_z + p_z - nu_eff * (w_xx + w_yy + w_zz)



    # Turbulent kinetic energy (k) equation
    production_k = nu_t * (u_x ** 2 + v_y ** 2 + w_z ** 2)  # Simple production term for k
    k_equation = u * k_x + v * k_y + w * k_z - epsilon + production_k

    # Dissipation rate (ε) equation
    epsilon_equation = u * epsilon_x + v * epsilon_y + w * epsilon_z - \
                      C1_eps * (epsilon / k) * production_k + C2_eps * (epsilon ** 2 / k)


    # Continuity equation
    continuity = u_x + v_y + w_z

    # Compute losses for Navier-Stokes, k, and ε equations
    momentum_loss = (torch.mean(f_u**2) + torch.mean(f_v**2)+ torch.mean(f_w**2))
    continuity_loss = torch.mean(continuity**2)
    loss_k = torch.mean(k_equation ** 2)
    loss_epsilon = torch.mean(epsilon_equation ** 2)

    return momentum_loss , continuity_loss , loss_k , loss_epsilon

def compute_BC_Loss(netu ,netv ,netw,x_b, y_b, z_b, u_b, v_b, w_b , p_b= None):
    u_b_pred = netu(torch.cat((x_b, y_b, z_b), dim=1))
    v_b_pred = netv(torch.cat((x_b, y_b, z_b), dim=1))
    w_b_pred = netw(torch.cat((x_b, y_b, z_b), dim=1))


    loss_u_b = torch.mean((u_b_pred - u_b) ** 2)
    loss_v_b = torch.mean((v_b_pred - v_b) ** 2)
    loss_w_b = torch.mean((w_b_pred - w_b) ** 2)
    #loss_p_b = torch.mean((p_b_pred - p_b) ** 2)

    boundary_loss = loss_u_b + loss_v_b + loss_w_b #+ loss_p_b
    return boundary_loss

    # Data Loss
def compute_data_Loss(netu ,netv ,netw,  x, y, z, u_exact, v_exact , w_exact ):
    u_pred = netu(torch.cat((x, y, z), dim=1))
    v_pred = netv(torch.cat((x, y, z), dim=1))
    w_pred = netw(torch.cat((x, y, z), dim=1))
    #p_pred = uvwp_pred[:, 3:4] if p_exact is not None else None

    loss_u = torch.mean((u_pred - u_exact) ** 2)
    loss_v = torch.mean((v_pred - v_exact) ** 2)
    loss_w = torch.mean((w_pred - w_exact) ** 2)
    #loss_p = torch.mean((p_pred - p_exact) ** 2) if p_exact is not None else 0

    data_loss = loss_u + loss_v + loss_w # + loss_p

    return data_loss


def total_loss(netu ,netv ,netw, netpke, x, y, z, u_exact, v_exact , w_exact ,x_b, y_b, z_b, u_b, v_b, w_b):
    # Weight balancing strategy: Normalize losses by magnitudes
    momentum_loss , continuity_loss , loss_k , loss_epsilon = compute_loss_PDE(netu ,netv ,netw, netpke, x, y,z, nu=1.5E-5/1.225, C_mu=0.09, C1_eps=1.44, C2_eps=1.92, sigma_k=1.0, sigma_eps=1.3)
    w_mom,w_cont, w_k , w_epsilon = compute_weight_PDE(momentum_loss , continuity_loss , loss_k , loss_epsilon, prev_mom = torch.tensor(1E11, requires_grad=False), prev_cont = torch.tensor(1E11, requires_grad=False), prev_k = torch.tensor(1E11, requires_grad=False) , prev_epsilon = torch.tensor(1E11, requires_grad=False))

    boundary_loss = compute_BC_Loss(netu ,netv ,netw,x_b, y_b, z_b, u_b, v_b, w_b)
    wu , wv , ww = compute_w_data(x,y,z)
    w_data = (wu + wv + ww) / 3
    data_loss = compute_data_Loss(netu ,netv ,netw, x, y, z, u_exact, v_exact , w_exact )
    loss = (momentum_loss * w_mom + continuity_loss * w_cont + boundary_loss  + data_loss * w_data +  loss_k * w_k + loss_epsilon * w_epsilon)

    return loss, momentum_loss * w_mom, continuity_loss * w_cont, boundary_loss , data_loss * w_data , loss_k * w_k , loss_epsilon * w_epsilon

#total_loss(netu ,netv ,netw, x, y, z, u_exact, v_exact , w_exact ,x_b, y_b, z_b, u_b, v_b, w_b )

loss, momentum_loss, continuity_loss , boundary_loss , data_loss, loss_k , loss_epsilon = total_loss(netu ,netv ,netw,netpke, x, y, z, u_exact, v_exact , w_exact ,x_b, y_b, z_b, u_b, v_b, w_b )
data_loss

### LBGFS optimizers

# Training parameters
epochs = 25000

# Define the optimizer
optu = optim.LBFGS(netu.parameters(), lr=0.001)#, max_iter=500, history_size=10)
optv = optim.Adam(netv.parameters(), lr=0.001)#, max_iter=500, history_size=10)
optw = optim.Adam(netv.parameters(), lr=0.001)#, max_iter=500, history_size=10)


for epoch in range(epochs):
    netu.train()
    netv.train()
    netw.train()

    def closure():
        optu.zero_grad()
        optv.zero_grad()
        optw.zero_grad()
        loss, momentum_loss, continuity_loss , boundary_loss , data_loss, loss_k , loss_epsilon = total_loss(netu ,netv ,netw,netpke, x, y, z, u_exact, v_exact , w_exact ,x_b, y_b, z_b, u_b, v_b, w_b )
        loss.backward()
        return loss  # Return the loss for the optimizer to use

    loss1 = optu.step(closure)  # The optimizer calls closure and gets the loss
    loss2 = optu.step(closure)  # The optimizer calls closure and gets the loss
    loss3 = optu.step(closure)  # The optimizer calls closure and gets the loss
    loss = (loss1 + loss2 + loss3) / 3

    if epoch % 500 == 0:
        print(f"Epoch {epoch}, Total Loss: {loss.item():.6f}, "
                  f"Momentum Loss: {momentum_loss.item():.6f}, "
                  f"Continuity Loss: {continuity_loss.item():.6f}, "
                  f"Data Loss: {data_loss.item():.6f}, "
                  f"Boundary Loss: {boundary_loss.item():.6f}")
        #print(time.time())

netu.eval()
netv.eval()
netw.eval()
with torch.no_grad():
    up = netu(torch.cat((x, y, z), dim=1))
    uv = netv(torch.cat((x, y, z), dim=1))
    uw = netw(torch.cat((x, y, z), dim=1))

def plot_results(exact, netu, x,y,z):
  u_p = netu(torch.cat((x, y, z), dim=1))
  plt.title("results PINN vd ground Truth")
  plt.plot(exact, label = "ground truth u")
  plt.plot(u_p.detach().numpy() , label = "PINN u")
  plt.legend()

plot_results(u_exact, netu, x, y, z)

plot_results(u_v, netu, x_v, y_v, z_v)

up = netu(torch.cat((x_v, y_v, z_v), dim=1))
plt.plot(u_v, "r")
plt.plot(up.detach().numpy(), "b")
#plt.ylim(-1,1)